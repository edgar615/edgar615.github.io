---
layout: post
title: Linux监控工具-iostat
date: 2019-10-30
categories:
    - linux
comments: true
permalink: linux-monitor-iostat.html
---

iostat是I/O statistics（输入/输出统计）的缩写，用来动态监视系统的磁盘操作活动。
通过iostat方便查看CPU、网卡、tty设备、磁盘、CD-ROM 等等设备的活动情况, 负载信息。

# 语法

	[root@ihorn-dev ~]# iostat --help
	Usage: iostat [ options ] [ <interval> [ <count> ] ]
	Options are:
	[ -c ] [ -d ] [ -h ] [ -k | -m ] [ -N ] [ -t ] [ -V ] [ -x ] [ -y ] [ -z ]
	[ -j { ID | LABEL | PATH | UUID | ... } ]
	[ [ -T ] -g <group_name> ] [ -p [ <device> [,...] | ALL ] ]
	[ <device> [...] | ALL ]

参数说明

    -C 显示CPU使用情况
    -d 显示磁盘使用情况
    -k 以 KB 为单位显示
    -m 以 M 为单位显示
    -N 显示磁盘阵列(LVM) 信息
    -n 显示NFS 使用情况
    -p[磁盘] 显示磁盘和分区的情况
    -t 显示终端和CPU的信息
    -x 显示详细信息
    -V 显示版本信息

# 示例

	[root@ihorn-dev ~]# iostat
	Linux 3.10.0-123.9.3.el7.x86_64 (ihorn-dev) 	12/23/2016 	_x86_64_	(4 CPU)
	
	avg-cpu:  %user   %nice %system %iowait  %steal   %idle
	           7.16    0.00    4.38    0.15    0.00   88.31
	
	Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn
	vda               9.76         0.17        76.21    1326593  579891120
	vdb               3.45         0.30        91.66    2313573  697435684
	dm-0              8.41         0.30        91.66    2312677  697435684
	dm-1              0.15         0.11         0.46     841696    3471701
	dm-2              0.00         0.00         0.00      29879       2880
	dm-3              0.00         0.00         0.00      15825       2304
	dm-4              0.11         0.01         0.34      58159    2576270
	dm-5              0.01         0.10         0.12     730393     88275

cpu属性值说明：

    %user：CPU处在用户模式下的时间百分比。
    %nice：CPU处在带NICE值的用户模式下的时间百分比。
    %system：CPU处在系统模式下的时间百分比。
    %iowait：CPU等待输入输出完成时间的百分比。
    %steal：管理程序维护另一个虚拟处理器时，虚拟CPU的无意识等待时间百分比。
    %idle：CPU空闲时间百分比。

注：如果%iowait的值过高，表示硬盘存在I/O瓶颈，%idle值高，表示CPU较空闲，如果%idle值高但系统响应慢时，有可能是CPU等待分配内存，此时应加大内存容量。%idle值如果持续低于10，那么系统的CPU处理能力相对较低，表明系统中最需要解决的资源是CPU。

disk属性值说明：

	tps：该设备每秒的传输次数（Indicate the number of transfers per second that were issued to the device.）。“一次传输”意思是“一次I/O请求”。多个逻辑请求可能会被合并为“一次I/O请求”。“一次传输”请求的大小是未知的。	
	kB_read/s：每秒从设备（drive expressed）读取的数据量；
	kB_wrtn/s：每秒向设备（drive expressed）写入的数据量；
	kB_read：读取的总数据量；
	kB_wrtn：写入的总数量数据量；
	这些单位都为Kilobytes。

# 显示详细信息

	[root@ihorn-dev ~]# iostat -x
	Linux 3.10.0-123.9.3.el7.x86_64 (ihorn-dev) 	12/23/2016 	_x86_64_	(4 CPU)
	
	avg-cpu:  %user   %nice %system %iowait  %steal   %idle
	           7.16    0.00    4.38    0.15    0.00   88.31
	
	Device:         rrqm/s   wrqm/s     r/s     w/s    rkB/s    wkB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util
	vda               0.00     5.09    0.02    9.74     0.17    76.21    15.66     0.02    1.74    3.17    1.74   0.64   0.62
	vdb               0.00     5.37    0.02    3.43     0.30    91.66    53.31     0.01    4.27    1.22    4.29   0.74   0.26
	dm-0              0.00     0.00    0.02    8.39     0.30    91.66    21.86     0.03    3.18    1.22    3.19   0.31   0.26
	dm-1              0.00     0.00    0.00    0.15     0.11     0.46     7.52     0.00    1.24    0.66    1.26   1.21   0.02
	dm-2              0.00     0.00    0.00    0.00     0.00     0.00    19.20     0.00    1.25    2.44    0.58   0.48   0.00
	dm-3              0.00     0.00    0.00    0.00     0.00     0.00    20.77     0.00    0.52    0.85    0.30   0.34   0.00
	dm-4              0.00     0.00    0.00    0.11     0.01     0.34     6.30     0.00    1.68    1.37    1.68   1.67   0.02
	dm-5              0.00     0.00    0.00    0.00     0.10     0.12    71.66     0.00    0.97    0.40    1.36   0.55   0.00


disk属性值说明：

- rrqm/s: 每秒进行 merge 的读操作数目。即 rmerge/s，（当系统调用需要读取数据的时候，VFS将请求发到各个FS，如果FS发现不同的读取请求读取的是相同Block的数据，FS会将这个请求合并Merge）；
- wrqm/s: 每秒进行 merge 的写操作数目。即 wmerge/s
- r/s: 每秒完成的读 I/O 设备次数。即 rio/s
- w/s: 每秒完成的写 I/O 设备次数。即 wio/s
- rsec/s: 每秒读扇区数。即 rsect/s
- wsec/s: 每秒写扇区数。即 wsect/s
- rkB/s: 每秒读K字节数。是 rsect/s 的一半，因为每扇区大小为512字节。
- wkB/s: 每秒写K字节数。是 wsect/s 的一半。
- avgrq-sz: 平均每次设备I/O操作的数据大小 (扇区)。
- avgqu-sz: 平均I/O队列长度。
- await: 平均每次设备I/O操作的等待时间 (毫秒)。这里可以理解为IO的响应时间，一般地系统IO响应时间应该低于5ms，如果大于10ms就比较大了。
- svctm: 平均每次设备I/O操作的服务时间 (毫秒)。
- %util: 一秒中有百分之多少的时间用于 I/O 操作，即被io消耗的cpu百分比，例如，如果统计间隔1秒，该设备有0.8秒在处理IO，而0.2秒闲置，那么该设备的%util = 0.8/1 = 80%，所以该参数暗示了设备的繁忙程度。一般地，如果该参数是100%表示设备已经接近满负荷运行了（当然如果是多磁盘，即使%util是100%，因为磁盘的并发能力，所以磁盘使用未必就到了瓶颈）。

备注：如果 %util 接近 100%，说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间；如果 await 远大于 svctm，说明I/O 队列太长，io响应太慢，则需要进行必要优化。如果avgqu-sz比较大，也表示有当量io在等待。

如果 %util 接近 100%，说明产生的I/O请求太多，I/O系统已经满负荷，该磁盘可能存在瓶颈。 idle小于70% IO压力就较大了，一般读取速度有较多的wait。 同时可以结合vmstat 查看查看b参数(等待资源的进程数)和wa参数(IO等待所占用的CPU时间的百分比，高过30%时IO压力高)。

另外 await 的参数也要多和 svctm 来参考。差的过高就一定有 IO 的问题。

avgqu-sz 也是个做 IO 调优时需要注意的地方，这个就是直接每次操作的数据的大小，如果次数多，但数据拿的小的话，其实 IO 也会很小。如果数据拿的大，才IO 的数据会高。也可以通过 avgqu-sz × ( r/s or w/s ) = rsec/s or wsec/s。也就是讲，读定速度是这个来决定的。

svctm 一般要小于 await (因为同时等待的请求的等待时间被重复计算了)，svctm 的大小一般和磁盘性能有关，CPU/内存的负荷也会对其有影响，请求过多也会间接导致 svctm 的增加。await 的大小一般取决于服务时间(svctm) 以及 I/O 队列的长度和 I/O 请求的发出模式。如果 svctm 比较接近 await，说明 I/O 几乎没有等待时间；如果 await 远大于 svctm，说明 I/O 队列太长，应用得到的响应时间变慢，如果响应时间超过了用户可以容许的范围，这时可以考虑更换更快的磁盘，调整内核 elevator 算法，优化应用，或者升级 CPU。

队列长度(avgqu-sz)也可作为衡量系统 I/O 负荷的指标，但由于 avgqu-sz 是按照单位时间的平均值，所以不能反映瞬间的 I/O 洪水。

别人一个不错的例子.(I/O 系统 vs. 超市排队)

举一个例子,我们在超市排队 checkout 时,怎么决定该去哪个交款台呢? 首当是看排的队人数,5个人总比20人要快吧? 除了数人头,我们也常常看看前面人购买的东西多少,如果前面有个采购了一星期食品的大妈,那么可以考虑换个队排了.还有就是收银员的速度了,如果碰上了连 钱都点不清楚的新手,那就有的等了.另外,时机也很重要,可能 5 分钟前还人满为患的收款台,现在已是人去楼空,这时候交款可是很爽啊,当然,前提是那过去的 5 分钟里所做的事情比排队要有意义 (不过我还没发现什么事情比排队还无聊的).

I/O 系统也和超市排队有很多类似之处:

	r/s+w/s 类似于交款人的总数
	平均队列长度(avgqu-sz)类似于单位时间里平均排队人的个数
	平均服务时间(svctm)类似于收银员的收款速度
	平均等待时间(await)类似于平均每人的等待时间
	平均I/O数据(avgrq-sz)类似于平均每人所买的东西多少
	I/O 操作率 (%util)类似于收款台前有人排队的时间比例.

设备IO操作:总IO(io)/s = r/s(读) +w/s(写)

平均等待时间=单个I/O服务器时间*(1+2+...+请求总数-1)/请求总数

每秒发出的I/0请求很多,但是平均队列就4,表示这些请求比较均匀,大部分处理还是比较及时。


rrqm/s
队列中每秒钟合并的读请求数量（当系统调用需要读取数据的时候，VFS将请求发到各个FS，如果FS发现不同的读取请求读取的是相同Block的数据，FS会将这个请求合并Merge）


wrqm/s
队列中每秒钟合并的写请求数量


r/s
每秒钟完成的读请求数量


w/s
每秒钟完成的写请求数量


rMB/s
每秒钟读取的数量，读IOPS=(rMB/s)  / (r/s)，IOPS:一次磁盘的连续读或者连续写称为一次磁盘 I/O，随机读写频繁的应用的关键衡量指标，IOPS = 1s/(寻道时间+旋转延迟+数据传输时间)


wMB/s
每秒钟写入的数量，写IOPS=(wMB/s)  / (w/s)


avgrq-sz
平均请求扇区的大小，平均每次请求的大小，avgrq-sz < 32K 随机存取为主。 avgrq-sz > 32K  顺序存储为主


avgqu-sz
平均请求队列的长度，此值越小越好，avgqu-sz > 2 可以认为存在I/O性能问题


await
平均每次请求的等待时间，单位毫秒，一般系统IO响应时间应该低于5ms，如果大于10ms就比较大了。等待时间包括了队列时间和服务时间，await和svctm越接近越好，代表几乎无需等待，反之差值越大，队列的时间越长，应用得到的响应时间变慢， 还可参考vmstat结果b参数（等待资源的进程数）和wa参数（IO等待所占用CPU时间百分比）


svctm
平均每次请求的服务时间，即磁盘读或写操作执行的时间，包括寻道，旋转时延，和数据传输等时间。（**寻道时间：**是指将读写磁头移动至正确的磁道上所需要的时间。寻道时间越短，I/O操作越快，目前磁盘的平均寻道时间一般在3－15ms。**旋转延迟：** 是指盘片旋转将请求数据所在扇区移至读写磁头下方所需要的时间。旋转延迟取决于磁盘转速，通常使用磁盘旋转一周所需时间的1/2表示。比如，7200 rpm的磁盘平均旋转延迟大约为60*1000/7200/2 = 4.17ms，而转速为15000 rpm的磁盘其平均旋转延迟约为2ms。**数据传输时间：** 是指完成传输所请求的数据所需要的时间，它取决于数据传输率，其值等于数据大小除以数据传输率。目前IDE/ATA能达到133MB/s，SATA II可达到300MB/s的接口数据传输率，数据传输时间通常远小于前两部分消耗时间，简单计算时可忽略。）  (r/s+w/s)*(svctm/1000)=util，如果util达到100%，那么此时svctm=1000/(r/s+w/s)，假设IOPS是1000，那么svctm大概在1毫秒左右，如果长时间大于这个数值，说明系统出了问题。


util
设备的利用率，如果util接近100%，则说明设备的能力趋向于饱和（如果是多磁盘，即使%util是100%，因为磁盘的并发能力，所以磁盘使用未必就到了瓶颈）

# 参考资料
http://linuxtools-rst.readthedocs.io/zh_CN/latest/tool/iostat.html

http://www.ha97.com/4546.html
