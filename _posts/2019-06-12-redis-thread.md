---
layout: post
title: redis线程模型
date: 2019-06-12
categories:
    - redis
comments: true
permalink: redis-thread.html
---

redis使用的单线程模型

1. redis 会将每个客户端都关联一个指令队列。客户端的指令通过队列来按顺序处理，先到先服务。
2. 在一个客户端的指令队列中的指令是顺序执行的，但是多个指令队列中的指令是无法保证顺序的
3. redis 同样也会为每个客户端关联一个响应队列，通过响应队列来顺序地将指令的返回结果回复给客户端。
4. 同样，一个响应队列中的消息可以顺序的回复给客户端，多个响应队列之间是无法保证顺序的。
5. 所有的客户端的队列中的指令或者响应，redis 每次都只能处理一个，同一时间绝对不会处理超过一个指令或者响应。

# 为什么redis能够快速执行

- redis 将所有数据放在内存中，绝大部分请求是纯粹的内存操作，内存的响应时长大约为 100 纳秒，这是 redis 的 QPS 过万的重要基础。
- 采用单线程,避免了不必要的上下文切换和竞争条件
- 非阻塞I/O，Redis采用epoll做为I/O多路复用技术的实现，再加上Redis自身的事件处理模型将epoll中的连接，读写，关闭都转换为了事件，不在I/O上浪费过多的时间。
- Redis采用单线程模型，每条命令执行如果占用大量时间，会造成其他线程阻塞，对于Redis这种高性能服务是致命的，所以Redis是面向高速执行的数据库。
- Redis为了高效的处理客户端的事件，并没有将持久化文件放在主线程里面进行处理，而是Redis在适当的时机fork子进程来异步的处理这种任务，Redis会fork子进程进行处理持久化文件操作（将数据写到RDB 文件中）。Redis还有一组异步任务处理线程，用于处理不需要主线程同步处理的工作，即处理一些低级别的事件（AOF文件重写）。


# IO多路复用
I/O multiplexing 这里面的 multiplexing 指的其实是在单个线程通过记录跟踪每一个Sock(I/O流)的状态来同时管理多个I/O流. 发明它的原因，是尽量多的提高服务器的吞吐能力。

参考下图(来源于参考资料)

![](/assets/images/posts/redis-thread/redis-thread-1.png)

在上图中，redis 需要处理 3 个 IO 请求，同时把 3 个请求的结果返回给客户端，所以总共需要处理 6 个 IO 事件，由于 redis 是单线程模型，同一时间只能处理一个 IO 事件，于是 redis 需要在合适的时间暂停对某个 IO 事件的处理，转而去处理另一个 IO 事件，这样 redis 就好比一个开关，当开关拨到哪个 IO 事件这个电路上，就处理哪个 IO 事件，其他 IO 事件就暂停处理了。这就是IO多路复用技术。

在系统底层，IO 多路复用有 3 种实现机制：

- select
- poll
- epoll（只有Linux支持，比如BSD上面对应的实现是kqueue）

**注意：select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间**

下面比较一下3种机制的区别

**用户态将文件描述符传入内核的方式**

- select：创建3个文件描述符集并拷贝到内核中,分别监听读、写、异常动作。**这里受到单个进程可以打开的fd数量限制,默认是1024**
- poll：将传入的struct pollfd结构体数组拷贝到内核中进行监听
- epoll：执行epoll_create会在内核的高速cache区中建立一颗红黑树以及就绪链表(该链表存储已经就绪的文件描述符)。接着用户执行的epoll_ctl函数添加文件描述符会在红黑树上增加相应的结点

**内核态检测文件描述符是否可读可写的方式**

- select：采用轮询方式,遍历所有fd,最后返回一个描述符读写操作是否就绪的mask掩码,根据这个掩码给fd_set赋值。 
- poll：同样采用轮询方式,查询每个fd的状态,如果就绪则在等待队列中加入一项并继续遍历。 
- epoll：采用回调机制。在执行epoll_ctl的add操作时,不仅将文件描述符放到红黑树上,而且也注册了回调函数,内核在检测到某文件描述符可读/可写时会调用回调函数,该回调函数将文件描述符放在就绪链表中。

**如何找到就绪的文件描述符并传递给用户态**

- select：将之前传入的fd_set拷贝传出到用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态,需要遍历来判断。 
- poll：将之前传入的fd数组拷贝传出用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态,需要遍历来判断。 
- epoll：epoll_wait只用观察就绪链表中有无数据即可,最后将链表的数据返回给数组并返回就绪的数量。内核将就绪的文件描述符放在传入的数组中,所以只用遍历依次处理即可。这里返回的文件描述符是通过mmap让内核和用户空间共享同一块内存实现传递的,减少了不必要的拷贝。

**继续重新监听时如何重复以上步骤**

- select：将新的监听文件描述符集合拷贝传入内核中,继续以上步骤。 
- poll：将新的struct pollfd结构体数组拷贝传入内核中,继续以上步骤。 
- epoll：无需重新构建红黑树,直接沿用已存在的即可。

通过以上步骤我们可以发现以下几点：

- select和poll的动作基本一致,只是poll采用链表来进行文件描述符的存储,而select采用fd标注位来存放,所以select会受到最大连接数的限制,而poll不会。
- select、poll、epoll虽然都会返回就绪的文件描述符数量。但是select和poll并不会明确指出是哪些文件描述符就绪,而epoll会。造成的区别就是,系统调用返回后,调用select和poll的程序需要遍历监听的整个文件描述符找到是谁处于就绪,而epoll则直接处理就行了。
- select、poll都需要将有关文件描述符的数据结构拷贝进内核,最后再拷贝出来。而epoll创建的有关文件描述符的数据结构本身就存于内核态中,系统调用返回时也采用mmap共享存储区,需要拷贝的次数大大减少。
- select、poll采用轮询的方式来检查文件描述符是否处于就绪态,而epoll采用回调机制。造成的结果就是,随着fd的增加,select和poll的效率会线性降低,而epoll不会受到太大影响,除非活跃的socket很多。
- 
最后总结一下,epoll比select和poll高效的原因主要有两点： 

1. 减少了用户态和内核态之间的文件描述符拷贝 
2. 减少了对就绪文件描述符的遍历

> 商城select、poll、epoll的区别完全复制自https://www.liangzl.com/get-article-detail-26853.html

适应场景

- select、poll适用于所监视的文件描述符数量较少的场景
- 当活动连接比较多的时候，epoll_wait的效率未必比select和poll高，因为此时回调函数被触发的过于频繁，因此epoll_wait适用于连接数量多，但活动连接较少的情况


# 参考资料

https://cloud.tencent.com/developer/article/1403767

https://www.zhihu.com/question/32163005

https://www.liangzl.com/get-article-detail-26853.html
