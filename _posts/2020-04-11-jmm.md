---
layout: post
title: JMM：Java内存模型
date: 2020-04-11
categories:
    - 多线程
comments: true
permalink: jmm.html
---

# 1. CPU三级缓存
计算机在执行程序的时候，每条指令都是在CPU中执行的，而执行的时候，又免不了要和数据打交道。而计算机上面的数据，是存放在主存（物理内存）当中的。但是随着CPU技术的发展，CPU的执行速度越来越快。而由于内存的技术并没有太大的变化，所以从内存中读取和写入数据的过程和CPU的执行速度比起来差距就会越来越大,这就导致CPU每次操作内存都要耗费很多等待时间。

为了避免内存成为计算机处理的瓶颈，我们在CPU和内存之间增加高速缓存：

![](/assets/images/posts/concurrency/concurrency-1.png)

**当程序在运行过程中，会将运算需要的数据从主存复制一份到CPU的高速缓存当中，那么CPU进行计算时就可以直接从它的高速缓存读取数据和向其中写入数据，当运算结束之后，再将高速缓存中的数据刷新到主存当中**

在高速缓存出现后不久，系统变得越来越复杂，高速缓存与主存之间的速度差异被拉大，直到加入了另一级缓存，新加入的这级缓存比第一缓存更大，但是更慢，而且经济上不合适，所以有了二级缓存，甚至有些系统已经拥有了三级缓存，于是就演变成了多级缓存。这三种缓存的技术难度和制造成本是相对递减的，所以其容量也是相对递增的。

![](/assets/images/posts/concurrency/concurrency-2.png)

L1最靠近CPU核心；L2其次；L3再次。在运行速度方面：L1最快、L2次快、L3最慢；在容量大小方面：L1最小、L2较大、L3最大

当CPU要读取一个数据时，首先从一级缓存中查找，如果没有找到再从二级缓存中查找，如果还是没有就从三级缓存或内存中查找。

## 1.1. 缓存一致性
多级缓存-缓存一致性（MESI），MESI是一个协议，这协议用于保证多个CPU cache之间缓存共享数据的一致性。它定义了CacheLine的四种数据状态，而CPU对cache的四种操作可能会产生不一致的状态。因此缓存控制器监听到本地操作与远程操作的时候需要对地址一致的CacheLine状态做出一定的修改，从而保证数据在多个cache之间流转的一致性。CacheLine的四种状态如下：

- M: Modified 修改，指的是该缓存行只被缓存在该CPU的缓存中，并且是被修改过的，因此他与主存中的数据是不一致的， 该缓存行中的数据需要在未来的某个时间点（允许其他CPU读取主存相应中的内容之前）写回主存，而当数据被写回主存之后，该缓存行的状态会变成E（独享）
- E：Exclusive 独享 缓存行只被缓存在该CPU的缓存中，是未被修改过的，与主存的数据是一致的，可以在任何时刻当有其他CPU读取该内存时，变成S（共享）状态，同样的当CPU修改该缓存行的内容时，会变成M（被修改）的状态
- S：Share 共享，当前CPU和其他CPU中都有共同数据，并且和主存中的数据一致；意味着该缓存行可能会被多个CPU进行缓存，并且各缓存中的数据与主存数据是一致的，当有一个CPU修改该缓存行时，在其他CPU中的该缓存行是可以被作废的，变成I(无效的) 状态
- I：Invalid 无效的，代表这个缓存是无效的，可能是有其他CPU修改了该缓存行；数据应该从主存中获取，其他CPU中可能有数据也可能无数据，当前CPU中的数据和主存被认为是不一致的；对于invalid而言，在MESI协议中采取的是写失效（write invalidate）。

CacheLine有四种数据状态（MESI），而引起数据状态转换的CPU cache操作也有四种：

- local read：读本地缓存中的数据
- local write：将数据写到本地缓存里面
- remote read：将内（主）存中的数据读取到缓存中来
- remote write：将缓存中的数据写回到主存里面去

在一个典型的多核系统中，每一个核都会有自己的缓存来共享主存总线，每个相应的CPU会发出读写（I/O）请求，而缓存的目的是为了减少CPU读写共享主存的次数。一个缓存除了在 Invalid 状态之外，都可以满足CPU的读请求。

一个写请求只有在该缓存行是M状态，或者E状态的时候才能够被执行。如果当前状态是处在S状态的时候，它必须先将缓存中的该缓存行变成无效的（Invalid）状态，这个操作通常作用于广播的方式来完成。这个时候它既不允许不同的CPU同时修改同一个缓存行，即使修改该缓存行不同位置的数据也是不允许的，这里主要解决的是缓存一致性的问题。一个处于M状态的缓存行它必须时刻监听所有试图读该缓存行相对就主存的操作，这种操作必须在缓存将该缓存行写回主存并将状态变成S状态之前被延迟执行。

一个处于S状态的缓存行也必须监听其它缓存使该缓存行无效或者独享该缓存行的请求，并将该缓存行变成无效（Invalid）。

一个处于E状态的缓存行也必须监听其它缓存读主存中该缓存行的操作，一旦有这种操作，该缓存行需要变成S状态。

因此，对于M和E两种状态而言总是精确的，他们在和该缓存行的真正状态是一致的。而S状态可能是非一致的，如果一个缓存将处于S状态的缓存行作废了，而另一个缓存实际上可能已经独享了该缓存行，但是该缓存却不会将该缓存行升迁为E状态，这是因为其它缓存不会广播他们作废掉该缓存行的通知，同样由于缓存并没有保存该缓存行的copy的数量，因此（即使有这种通知）也没有办法确定自己是否已经独享了该缓存行。

从上面的意义看来E状态是一种投机性的优化：如果一个CPU想修改一个处于S状态的缓存行，总线事务需要将所有该缓存行的copy变成invalid状态，而修改E状态的缓存不需要使用总线事务。

## 1.2. 指令重排序
指令重排序处理器为提高运算速度而做出违背代码原有顺序的优化。
例如代码
```
a=10;
b=200;
result=a*b;
```
在CPU重排序后可能变为
```
b=200;
a=10;
result=a*b;
```

可以看到CPU重排序执行优化后的代码并不会对计算结果造成影响，但这也只是其中一种没被影响的情况而已。在单核时代，处理器保证做出的优化不会导致执行结果远离预期目标。但是在多核环境下则并非如此，因为在多核环境下同时会有多个核心在执行指令，每个核心的指令都可能被乱序。另外处理器还引入了L1、L2等多级缓存机制，而每个核心都有自己的缓存，这就导致了逻辑次序上后写入的数据未必真的写入了。如果我们不做任何防护措施，那么处理器最终处理的结果可能与我们代码的逻辑结果大不相同。比如我们在一个核心上执行数据写入操作，并在最后写一个标记用来表示之前的数据已经准备好了。然后从另外一个核心上通过判断这个标记来判定所需要的数据是否已准备就绪，这种做法就存在一定的风险，标记位可能先被写入，而数据并未准备完成，这个未完成既有可能是没有计算完成，也有可能是缓存没有被及时刷新到主存之中，这样最终就会导致另外的核心使用了错误的数据，所以我们才经常在多线程的情况下保证线程安全。

# 2.Java内存模型
java 内存模型（JMM）是一种抽象的概念，并不真实存在，它描述了一组规则或规范，通过这组规范定义了程序中各个变量（包括实例字段、静态字段和构成数组对象的元素）的访问方式。试图屏蔽各种硬件和操作系统的内存访问差异，以实现让 Java 程序在各种平台下都能达到一致的内存访问效果

Java内存模型把Java虚拟机内部划分为线程栈和堆。

![](/assets/images/posts/concurrency/concurrency-3.png)

- Head（堆）：java里的堆是一个运行时的数据区，堆是由垃圾回收机制来负责的。堆的优势是可以动态地分配内存大小，生存期也不必事先告诉编译器，因为它是在运行时动态分配内存的，而且Java的垃圾回收机制也会自动的收走那些不再使用的数据。但是它也有缺点，由于是运行时动态分配内存，因此它的存取速度相对要慢一些。
- Stack（栈）：栈的优势是存取速度比堆要快，仅次于计算机里的寄存器，栈的数据是可以共享的。而栈的缺点则是存在栈中的数据的大小以及生存期必须是确定的，缺乏一些灵活性，所以栈中主要用来存储一些基本数据类型的变量，比如：int，short，long，byte，double，float，boolean，char以及对象句柄等。

每一个运行在Java虚拟机里的线程都拥有自己的线程栈。这个线程栈包含了这个线程调用的方法当前执行点相关的信息。一个线程仅能访问自己的线程栈。一个线程创建的本地变量对其它线程不可见，仅自己可见。即使两个线程执行同样的代码，这两个线程任然在在自己的线程栈中的代码来创建本地变量。因此，每个线程拥有每个本地变量的独有版本。

所有原始类型的本地变量都存放在线程栈上，因此对其它线程不可见。一个线程可能向另一个线程传递一个原始类型变量的拷贝，但是它不能共享这个原始类型变量自身。

堆上包含在Java程序中创建的所有对象，无论是哪一个对象创建的。这包括原始类型的对象版本。如果一个对象被创建然后赋值给一个局部变量，或者用来作为另一个对象的成员变量，这个对象任然是存放在堆上。

一个本地变量也可能是指向一个对象的引用，这种情况下这个保存对象引用的本地变量是存放在线程栈上的，但是对象本身则是存放在堆上的。

一个对象可能包含方法，而这些方法可能包含着本地变量，这些本地变量仍然是存放在线程栈上的。即使这些方法所属的对象是存放在堆上的。一个对象的成员变量，可能会随着所属对象而存放在堆上，不管这个成员变量是原始类型还是引用类型。静态成员变量则是随着类的定义一起存放在堆上。

存放在堆上的对象，可以被持有这个对象的引用的线程访问。当一个线程可以访问某个对象时，它也可以访问该对象的成员变量。如果两个线程同时调用同一个对象上的同一个方法，那么它们都将会访问这个方法中的成员变量，但是每一个线程都拥有这个成员变量的私有拷贝。

## 2.1. 主内存与工作内存

处理器上的寄存器的读写的速度比内存快几个数量级，为了解决这种速度矛盾，在它们之间加入了高速缓存。

加入高速缓存带来了一个新的问题：缓存一致性。如果多个缓存共享同一块主内存区域，那么多个缓存的数据可能会不一致，需要一些协议来解决这个问题。

![](/assets/images/posts/jmm/jmm-1.png)

所有的变量都**存储在主内存中，每个线程还有自己的工作内存**，工作内存存储在高速缓存或者寄存器中，保存了该线程使用的变量的主内存副本拷贝。

线程只能直接操作工作内存中的变量，不同线程之间的变量值传递需要通过主内存来完成。

![](/assets/images/posts/jmm/jmm-2.png)

## 2.2. 数据存储类型以及操作方式

方法中的基本类型本地变量将直接存储在工作内存的栈帧结构中；

引用类型的本地变量：引用存储在工作内存，实际存储在主内存；

成员变量、静态变量、类信息均会被存储在主内存中；

主内存共享的方式是线程各拷贝一份数据到工作内存中，操作完成后就刷新到主内存中。

# 2.3. 内存间交互操作

线程间如果要完成变成的同步和共享，必须经历下面2个步骤。

1. 线程A必须要把线程A的工作内存更新过的变量刷新到主内存去。

2. 线程B到主内存中去读取线程A更新过的共享变量

这些通讯操作是被JMM屏蔽的，要保证变量的线程安全共享 需要使用Java的同步块(synchonrized），或者其他并发工具。这里强调的是安全共享，在不加同步块，和并发工具的情况下，变量也是可以被共享的，只是不能保证读都最新数据，就是常说的 脏读，错读等

![](/assets/images/posts/jmm/jmm-4.png)

Java 内存模型定义了 8 个操作来完成主内存和工作内存的交互操作。

![](/assets/images/posts/jmm/jmm-3.png)

- read：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的load动作使用
- load：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中
- use：作用于工作内存的变量，它把工作内存中一个变量的值传递给执行引擎，每当虚拟机遇到一个需要使用到变量的值的字节码指令时将会执行这个操作
- assign：作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作
- store：作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的write操作使用
- write：作用于主内存的变量，它把store操作从工作内存中得到的变量的值放入主内存的变量中
- lock：作用于主内存的变量，它把一个变量标识为一条线程独占的状态
- unlock：作用于主内存的变量，它把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定

网上找了个更完整的图

![](/assets/images/posts/jmm/jmm-5.png)

JMM规定如果要把一个变量从主内存复制到工作内存，那就要顺序地执行read和load操作，如果要把变量从工作内存同步回主内存，就要顺序地执行store和write操作。

注意，Java内存模型只要求上述两个操作必须按顺序执行，而没有保证是连续执行。也就是说，read与load之间、store与write之间是可插入其他指令的，如对主内存中的变量a、b进行访问时，一种可能出现顺序是read a、read b、load b、load a。    

**Java内存模型还规定了在执行上述8种基本操作时，必须满足如下规则：**

- 不允许 read 和 load、store 和 write 操作之一单独出现
- 不允许一个线程丢弃它的最近 assign 的操作，即变量在工作内存中改变了之后必须同步到主内存中
- 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步回主内存中
- 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load 或 assign）的变量。即就是对一个变量实施 use 和 store 操作之前，必须先执行过了 assign 和 load 操作。
- 一个变量在同一时刻只允许一条线程对其进行lock操作，lock 和 unlock必须成对出现
- 如果对一个变量执行 lock 操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行 load 或 assign 操作初始化变量的值
- 如果一个变量事先没有被 lock 操作锁定，则不允许对它执行 unlock 操作；也不允许去 unlock 一个被其他线程锁定的变量。
- 对一个变量执行 unlock 操作之前，必须先把此变量同步到主内存中（执行 store 和 write 操作）。

# 3. Volatile

关键字volatile可以说是Java虚拟机提供的最轻量级的同步机制. 当一个变量定义为volatile之后，它将具备两种特性，第一是保证此变量对所有线程的可见性. 第二个语义是禁止指令重排序优化.

这里的“可见性”是指当一条线程修改了这个变量的值，新值对于其他线程来说是可以立即得知的。而普通变量不能做到这一点，普通变量的值在线程间传递均需要通过主内存来完成，例如，线程A修改一个普通变量的值。需要注意的是，volatile只是保证了可见性，很容易误解为volatile变量在各个线程中是一致的，所以基于volatile变量的运算在并发下是安全的。但是Java里面的运算并非原子操作，各个工作区的volaitile的变量可能存在不一致的情况，导致volatile变量的运算在并发下一样是不安全的。

# 4. 局部变量
Java线程可以拥有自己的操作数栈，程序计数器、局部变量表等资源；我们都知道，多个线程同时访问共享变量的时候，会导致数据不一致性等并发问题；但是 Java 方法里面的局部变量是不存在并发问题的。

**局部变量的作用域是方法内部的，当方法执行完了，局部变量也就销毁了，也就是说局部变量应该是和方法同生共死的。**

当调用方法时，会创建新的栈帧，并压入调用栈；当方法返回时，对应的栈帧就会被自动弹出。也就是说，栈帧和方法是同生共死的。

**方法的调用就是压栈和出栈的过程，而在Java中的方法的局部变量又是存储在栈帧中，而每个线程都有自己独立的调用栈，所以局部变量不存在并发问题**

![](/assets/images/posts/concurrency/concurrency-7.png)

# 5. 内存模型三大特性
## 5.1. 原子性

Java 内存模型保证了 read、load、use、assign、store、write、lock 和 unlock 操作具有原子性，例如对一个 int 类型的变量执行 assign 赋值操作，这个操作就是原子性的。但是 Java 内存模型允许虚拟机将没有被 volatile 修饰的 64 位数据（long，double）的读写操作划分为两次 32 位的操作来进行，即 load、store、read 和 write 操作可以不具备原子性。

如果应用场景需要一个更大范围的原子性保证（经常会遇到），Java内存模型还提供了lock和unlock操作来满足这种需求，尽管虚拟机未把lock和unlock操作直接开放给用户使用，但是却提供了更高层次的字节码指令monitorenter和monitorexit来隐式地使用这两个操作，这两个字节码指令反映到Java代码中就是同步块——synchronized关键字，因此在synchronized块之间的操作也具备原子性。

## 5.2. 可见性

可见性是指当一个线程修改了共享变量的值，其他线程能够立即得知这个修改。Java内存模型是通过在变量修改后将新值同步回主内存，在变量读取前从主内存刷新变量值这种依赖主内存作为传递媒介的方式来实现可见性的，无论是普通变量还是volatile变量都是如此，普通变量与volatile变量的区别是，volatile的特殊规则保证了新值能立即同步到主内存，以及每次使用前立即从主内存刷新。因此，可以说volatile保证了多线程操作时变量的可见性，而普通变量则不能保证这一点。

除了volatile之外，Java还有两个关键字能实现可见性，即synchronized和final。同步块的可见性是由“对一个变量执行unlock操作之前，必须先把此变量同步回主内存中（执行store、write操作）”这条规则获得的，而final关键字的可见性是指：被final修饰的字段在构造器中一旦初始化完成，并且构造器没有把“this”的引用传递出去（this引用逃逸是一件很危险的事情，其他线程有可能通过这个引用访问到“初始化了一半”的对象），那在其他线程中就能看见final字段的值。

## 5.3. 有序性

有序性是指：如果在本线程内观察，所有的操作都是有序的；如果在一个线程中观察另一个线程，所有的操作都是无序的。

前半句是指“线程内表现为串行的语义”（Within-Thread As-If-Serial Semantics），后半句是指“指令重排序”现象和“工作内存与主内存同步延迟”现象。

重排序过程不会影响到单线程程序的执行，却会影响到多线程并发执行的正确性。

Java语言提供了volatile和synchronized两个关键字来保证线程之间操作的有序性

- volatile 关键字通过添加内存屏障的方式来禁止指令重排，即重排序时不能把后面的指令放到内存屏障之前。
- synchronized 它保证每个时刻只有一个线程执行同步代码，相当于是让线程顺序执行同步代码。

### 5.3.1. 先行先发原则

如果Java内存模型中所有的有序性都仅仅靠volatile和synchronized来完成，那么有一些操作将会变得很烦琐，但是我们在编写Java并发代码的时候并没有感觉到这一点，这是因为Java语言中有一个“先行发生”（happens-before）的原则。这个原则非常重要，它是判断数据是否存在竞争、线程是否安全的主要依据，依靠这个原则，我们可以让一个操作**无需控制**就能先于另一个操作完成。

由于**指令重排序**的存在，两个操作之间有happen-before关系，**并不意味着前一个操作必须要在后一个操作之前执行。 仅仅要求前一个操作的执行结果对于后一个操作是可见的，并且前一个操作按顺序** 排在第二个操作之前。

### 5.3.2. 单一线程原则（程序员顺序规则）

Single Thread rule 在一个线程内，在程序前面的操作先行发生于后面的操作。

![](/assets/images/posts/jmm/jmm-6.png)

### 5.3.3. 管程锁定规则（监视器锁规则）

Monitor Lock Rule 一个 unlock（解锁） 操作先行发生于后面对同一个锁的 lock（加锁） 操作。

![](/assets/images/posts/jmm/jmm-7.png)

### 5.3.4. volatile 变量规则

Volatile Variable Rule 对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。

![](/assets/images/posts/jmm/jmm-8.png)

### 5.3.5. 线程启动规则

Thread Start Rule Thread 对象的start()方法调用先行发生于此线程的每一个动作。

![](/assets/images/posts/jmm/jmm-9.png)

### 5.3.6. 线程加入规则

Thread Join Rule Thread 对象的结束先行发生于 join() 方法返回。

![](/assets/images/posts/jmm/jmm-10.png)

### 5.3.7. 线程中断规则

Thread Interruption Rule 对线程 interrupt() 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 interrupted() 方法检测到是否有中断发生。

### 5.3.8. 对象终结规则

Finalizer Rule 一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize() 方法的开始。

### 5.3.9. 传递性

Transitivity 如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那么操作 A 先行发生于操作 C。

# 6. 内存屏障

处理器都支持一定的内存屏障(memory barrier)或栅栏(fence)来控制重排序和数据在不同的处理器间的可见性。例如，CPU将数据写回时，会将store请求放入write buffer中等待flush到内存，可以通过插入barrier的方式防止这个store请求与其他的请求重排序、保证数据的可见性。可以用一个生活中的例子类比屏障，例如坐地铁的斜坡式电梯时，大家按顺序进入电梯，但是会有一些人从左侧绕过去，这样出电梯时顺序就不相同了，如果有一个人携带了一个大的行李堵住了（屏障），则后面的人就不能绕过去了:)。另外这里的barrier和GC中用到的write barrier是不同的概念。

**内存屏障的分类**

几乎所有的处理器都支持一定粗粒度的barrier指令，通常叫做Fence(栅栏、围墙)，能够保证在fence之前发起的load和store指令都能严格的和fence之后的load和store保持有序。通常按照用途会分为下面四种barrier

**LoadLoad Barriers**

指令顺序如：Load1; LoadLoad; Load2;

保证Load1的数据在Load2及之后的load前加载

**StoreStore Barriers**

指令顺序如：Store1; StoreStore; Store2

保证Store1的数据先于Store2及之后的数据 在其他处理器可见

**LoadStore Barriers**

指令顺序如：Load1; LoadStore; Store2

保证Load1的数据的加载在Store2和之后的数据flush前

**StoreLoad Barriers**

指令顺序如：Store1; StoreLoad; Load2

保证Store1的数据对其他处理器变得可见(如flush到内存)先于Load2和之后的load的数据的加载。StoreLoad Barrier能够防止load读取到旧数据而不是最近其他处理器写入的数据。StoreLoad Barriers会使该屏障之前的所有内存访问指令完成之后，才执行该屏障之后的内存访问指令

StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他3个屏障的效果。现代的多处理器大多支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中（Buffer Fully Flush）。

举个例子说明，假设有一组 CPU 指令：

- **Store 表示“存储指令”**
- **Load 表示“读取指令”**
- **StoreLoad 代表“写读内存屏障”**

![](/assets/images/posts/jmm/jmm-11.jpg)

StoreLoad 屏障之前的 Store 指令，无法与 StoreLoad 屏障之后的 Load 指令进行交换位置，即重排序。

但是 StoreLoad 屏障之前和之后的指令是可以互换位置的，即 Store1 可以和 Store2 互换，Load2 可以和 Load3 互换。

volatile读前插读屏障，写后加写屏障，避免CPU重排导致的问题，实现多线程之间数据的可见性。

> x86架构为例，JVM对volatile变量的处理如下：
> 
> -  在写volatile变量v之后，插入一个sfence。这样，sfence之前的所有store（包括写v）不会被重排序到sfence之后，sfence之后的所有store不会被重排序到sfence之前，禁用跨sfence的store重排序；且sfence之前修改的值都会被写回缓存，并标记其他CPU中的缓存失效。
> -  在读volatile变量v之前，插入一个lfence。这样，lfence之后的load（包括读v）不会被重排序到lfence之前，lfence之前的load不会被重排序到lfence之后，禁用跨lfence的load重排序；且lfence之后，会首先刷新无效缓存，从而得到最新的修改值，与sfence配合保证内存可见性。

# 7. 参考资料

https://cloud.tencent.com/developer/article/1429393

https://www.jianshu.com/p/64240319ed60
