---
layout: post
title: Linux IO（2）- I/O多路复用
date: 2019-09-16
categories:
    - linux
comments: true
permalink: linux-io-multiplexing.html
---

应用程序通常需要处理来自多条事件流中的事件，比如我现在用的电脑，需要同时处理键盘鼠标的输入、中断信号等等事件，再比如web服务器如nginx，需要同时处理来来自N个客户端的事件。

>  逻辑控制流在时间上的重叠叫做 **并发**

而CPU单核在同一时刻只能做一件事情，一种解决办法是对CPU进行时分复用(多个事件流将CPU切割成多个时间片，不同事件流的时间片交替进行)。在计算机系统中，我们用线程或者进程来表示一条执行流，通过不同的线程或进程在操作系统内部的调度，来做到对CPU处理的时分复用。这样多个事件流就可以并发进行，不需要一个等待另一个太久，在用户看起来他们似乎就是并行在做一样。

但凡事都是有成本的。线程/进程也一样，有这么几个方面：

1. 线程/进程创建成本
2. CPU切换不同线程/进程成本
3. 多线程的资源竞争

有没有一种可以在单线程/进程中处理多个事件流的方法呢？一种答案就是IO多路复用。

因此IO多路复用解决的本质问题是在**用更少的资源完成更多的事**。

**I/O multiplexing 这里面的 multiplexing 指的其实是在单个线程通过记录跟踪每一个Sock(I/O流)的状态来同时管理多个I/O流**. 发明它的原因，是尽量多的提高服务器的吞吐能力。

在系统底层，IO 多路复用有 3 种实现机制：

- select
- poll
- epoll（只有Linux支持，比如BSD上面对应的实现是kqueue）

**注意：select，poll，epoll本质上都是同步I/O，因为他们都需要在读写事件就绪后自己负责进行读写，也就是说这个读写过程是阻塞的，而异步I/O则无需自己负责进行读写，异步I/O的实现会负责把数据从内核拷贝到用户空间**

下面比较一下3种机制的区别

**用户态将文件描述符传入内核的方式**

- select：创建3个文件描述符集并拷贝到内核中,分别监听读、写、异常动作。**这里受到单个进程可以打开的fd数量限制,默认是1024**
- poll：将传入的struct pollfd结构体数组拷贝到内核中进行监听
- epoll：执行epoll_create会在内核的高速cache区中建立一颗红黑树以及就绪链表(该链表存储已经就绪的文件描述符)。接着用户执行的epoll_ctl函数添加文件描述符会在红黑树上增加相应的结点

**内核态检测文件描述符是否可读可写的方式**

- select：**采用轮询方式**,遍历所有fd,最后返回一个描述符读写操作是否就绪的mask掩码,根据这个掩码给fd_set赋值。 
- poll：**同样采用轮询方式**,查询每个fd的状态,如果就绪则在等待队列中加入一项并继续遍历。 
- epoll：**采用回调机制**。在执行epoll_ctl的add操作时,不仅将文件描述符放到红黑树上,而且也注册了回调函数,内核在检测到某文件描述符可读/可写时会调用回调函数,该回调函数将文件描述符放在就绪链表中。

**如何找到就绪的文件描述符并传递给用户态**

- select：将之前传入的fd_set拷贝传出到用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态,需要遍历来判断。 
- poll：将之前传入的fd数组拷贝传出用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态,需要遍历来判断。 
- epoll：epoll_wait只用观察就绪链表中有无数据即可,最后将链表的数据返回给数组并返回就绪的数量。内核将就绪的文件描述符放在传入的数组中,所以只用遍历依次处理即可。这里返回的文件描述符是通过mmap让内核和用户空间共享同一块内存实现传递的,减少了不必要的拷贝。

**继续重新监听时如何重复以上步骤**

- select：将新的监听文件描述符集合拷贝传入内核中,继续以上步骤。 
- poll：将新的struct pollfd结构体数组拷贝传入内核中,继续以上步骤。 
- epoll：无需重新构建红黑树,直接沿用已存在的即可。

通过以上步骤我们可以发现以下几点：

- select和poll的动作基本一致,只是poll采用链表来进行文件描述符的存储,而select采用fd标注位来存放,所以select会受到最大连接数的限制,而poll不会。
- select、poll、epoll虽然都会返回就绪的文件描述符数量。但是select和poll并不会明确指出是哪些文件描述符就绪,而epoll会。造成的区别就是,系统调用返回后,调用select和poll的程序需要遍历监听的整个文件描述符找到是谁处于就绪,而epoll则直接处理就行了。
- select、poll都需要将有关文件描述符的数据结构拷贝进内核,最后再拷贝出来。而epoll创建的有关文件描述符的数据结构本身就存于内核态中,系统调用返回时也采用mmap共享存储区,需要拷贝的次数大大减少。
- select、poll采用轮询的方式来检查文件描述符是否处于就绪态,而epoll采用回调机制。造成的结果就是,随着fd的增加,select和poll的效率会线性降低,而epoll不会受到太大影响,除非活跃的socket很多。
- 最后总结一下,epoll比select和poll高效的原因主要有两点： 

1. 减少了用户态和内核态之间的文件描述符拷贝 
2. 减少了对就绪文件描述符的遍历

适应场景

- select、poll适用于所监视的文件描述符数量较少的场景
- 当活动连接比较多的时候，epoll_wait的效率未必比select和poll高，因为此时回调函数被触发的过于频繁，因此epoll_wait适用于连接数量多，但活动连接较少的情况

# 参考资料

https://www.liangzl.com/get-article-detail-26853.html